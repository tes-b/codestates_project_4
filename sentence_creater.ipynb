{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "from konlpy.tag import Komoran\n",
    "from konlpy.tag import Mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# komoran = Komoran()\n",
    "# mecab = Mecab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content\n",
      "화질이 왤케 안좋지..\n",
      "나만그런가..\n",
      "앗..\n",
      "네 그거함수\n",
      "편미분한거\n",
      "??\n",
      "ㅋㅋㅋ\n",
      "줌이 영상공유는 더낫네요 ㅋㅋ\n",
      "컨트롤 a 딜리트\n",
      "그르게요\n",
      "저게 그래서 뭐에요?\n",
      "글쿠나....\n",
      "마우스로 왤케 잘그리시지\n",
      "??\n",
      "오른쪽으로가는게 좌극한이에여?\n",
      "하...\n",
      "모르는 단어 넘많아\n",
      "교수님 미대에서 오신듯\n",
      "ㅋㅋㅋㅋㅋ\n",
      "최초공개!\n",
      "또 최초공개!\n",
      "아 0?\n",
      "zero가 되\n"
     ]
    }
   ],
   "source": [
    "text = open(\"data/clean/ts.txt\", 'rb').read().decode(encoding='utf-8')\n",
    "print(text[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Content\\n화질이 왤케 안좋지..\\n나만그런가..\\n앗..\\n네 그거함수\\n편미분한거\\n??\\nㅋㅋㅋ\\n줌이 영상공유는 더낫네요 ㅋㅋ\\n컨트롤 a 딜리트\\n그르게요\\n저게 그래서 뭐에요?\\n글쿠나....\\n마우스로 왤케 잘그리시지\\n??\\n오른쪽으로가는게 좌극한이에여?\\n하...\\n모르는 단어 넘많아\\n교수님 미대에서 오신듯\\nㅋㅋㅋㅋㅋ\\n최초공개!\\n또 최초공개!\\n아 0?\\nzero가 되'\n"
     ]
    }
   ],
   "source": [
    "print(repr(text[:200]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68813"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', '#', '%', '&', \"'\", '(', ')'] 1163\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "print(vocab[:10], len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1163"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "len(char2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# index -> character로 변환하는 사전 \n",
    "idx2char = np.array(vocab)\n",
    "idx2char[49]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68813"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_as_int = np.array([char2idx[c] for c in text])\n",
    "len(text_as_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(text_as_int)):\n",
    "    if(i == 0): continue\n",
    "    if(len(text_as_int) % i) == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X,Y 데이터셋 생성\n",
    "\n",
    "# 단일 입력에 대해 원하는 문장의 최대 길이\n",
    "window_size = 100\n",
    "shuffle_buffer = len(text_as_int)\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windowed Dataset을 만듭니다\n",
    "def windowed_dataset(series, window_size, shuffle_buffer, batch_size):\n",
    "    series = tf.expand_dims(series, -1)\n",
    "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
    "    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda x : x.batch(window_size + 1))\n",
    "    ds = ds.shuffle(shuffle_buffer)\n",
    "    ds = ds.map(lambda x: (x[:-1], x[1:]),\n",
    "                num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    "                )\n",
    "    return ds.repeat().batch(batch_size, drop_remainder=True).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 64\n",
    "\n",
    "# train_data = tf.data.Dataset.from_tensor_slices((train_feature, train_label))\n",
    "\n",
    "# train_data = train_data.repeat().batch(batch_size, drop_remainder=True)\n",
    "\n",
    "# steps_per_epoch = len(train_feature) // batch_size \n",
    "\n",
    "# model.fit(train_data, epochs=10, steps_per_epoch = steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-05 12:41:51.218620: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-05 12:41:51.562680: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5250 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "train_data = windowed_dataset(np.array(text_as_int), \n",
    "                            window_size=window_size, \n",
    "                            shuffle_buffer=shuffle_buffer, \n",
    "                            batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_length = [i for i,_ in enumerate(train_data)][-1] + 1\n",
    "# dataset_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문자로 된 어휘 사전의 크기\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# 임베딩 차원\n",
    "embedding_dim = 256\n",
    "\n",
    "# RNN 유닛 개수\n",
    "rnn_units = 1024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1163"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, embedding_dim,batch_input_shape=[batch_size, None]))\n",
    "# model.add(tf.keras.layers.Embedding(vocab_size, embedding_dim,input_length=window_size))\n",
    "model.add(tf.keras.layers.LSTM(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'))\n",
    "model.add(tf.keras.layers.Dense(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (64, None, 256)           297728    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (64, None, 1024)          5246976   \n",
      "                                                                 \n",
      " dense (Dense)               (64, None, 1163)          1192075   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,736,779\n",
      "Trainable params: 6,736,779\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = './models/my_checkpt.ckpt'\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True,\n",
    "    monitor='loss',\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss, metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# steps_per_epoch = dataset_length // batch_size\n",
    "# steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-05 12:41:57.488697: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1/1600 [..............................] - ETA: 2:42:47 - loss: 7.0593 - acc: 0.0000e+00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-05 12:41:58.029633: I tensorflow/stream_executor/cuda/cuda_blas.cc:1774] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - ETA: 0s - loss: 1.8168 - acc: 0.6373\n",
      "Epoch 00001: loss improved from inf to 1.81678, saving model to ./models/my_checkpt.ckpt\n",
      "1600/1600 [==============================] - 117s 70ms/step - loss: 1.8168 - acc: 0.6373\n",
      "Epoch 2/10\n",
      "1600/1600 [==============================] - ETA: 0s - loss: 0.2642 - acc: 0.9554\n",
      "Epoch 00002: loss improved from 1.81678 to 0.26424, saving model to ./models/my_checkpt.ckpt\n",
      "1600/1600 [==============================] - 111s 69ms/step - loss: 0.2642 - acc: 0.9554\n",
      "Epoch 3/10\n",
      "1600/1600 [==============================] - ETA: 0s - loss: 0.2030 - acc: 0.9631\n",
      "Epoch 00003: loss improved from 0.26424 to 0.20301, saving model to ./models/my_checkpt.ckpt\n",
      "1600/1600 [==============================] - 115s 72ms/step - loss: 0.2030 - acc: 0.9631\n",
      "Epoch 4/10\n",
      "1600/1600 [==============================] - ETA: 0s - loss: 0.1791 - acc: 0.9667\n",
      "Epoch 00004: loss improved from 0.20301 to 0.17907, saving model to ./models/my_checkpt.ckpt\n",
      "1600/1600 [==============================] - 111s 69ms/step - loss: 0.1791 - acc: 0.9667\n",
      "Epoch 5/10\n",
      "1600/1600 [==============================] - ETA: 0s - loss: 0.1631 - acc: 0.9690\n",
      "Epoch 00005: loss improved from 0.17907 to 0.16307, saving model to ./models/my_checkpt.ckpt\n",
      "1600/1600 [==============================] - 114s 72ms/step - loss: 0.1631 - acc: 0.9690\n",
      "Epoch 6/10\n",
      "1600/1600 [==============================] - ETA: 0s - loss: 0.1537 - acc: 0.9704\n",
      "Epoch 00006: loss improved from 0.16307 to 0.15373, saving model to ./models/my_checkpt.ckpt\n",
      "1600/1600 [==============================] - 111s 69ms/step - loss: 0.1537 - acc: 0.9704\n",
      "Epoch 7/10\n",
      "1600/1600 [==============================] - ETA: 0s - loss: 0.1458 - acc: 0.9716\n",
      "Epoch 00007: loss improved from 0.15373 to 0.14582, saving model to ./models/my_checkpt.ckpt\n",
      "1600/1600 [==============================] - 114s 72ms/step - loss: 0.1458 - acc: 0.9716\n",
      "Epoch 8/10\n",
      "1600/1600 [==============================] - ETA: 0s - loss: 0.1407 - acc: 0.9723\n",
      "Epoch 00008: loss improved from 0.14582 to 0.14066, saving model to ./models/my_checkpt.ckpt\n",
      "1600/1600 [==============================] - 111s 69ms/step - loss: 0.1407 - acc: 0.9723\n",
      "Epoch 9/10\n",
      "1600/1600 [==============================] - ETA: 0s - loss: 0.1357 - acc: 0.9730\n",
      "Epoch 00009: loss improved from 0.14066 to 0.13571, saving model to ./models/my_checkpt.ckpt\n",
      "1600/1600 [==============================] - 114s 71ms/step - loss: 0.1357 - acc: 0.9730\n",
      "Epoch 10/10\n",
      "1600/1600 [==============================] - ETA: 0s - loss: 0.1327 - acc: 0.9734\n",
      "Epoch 00010: loss improved from 0.13571 to 0.13266, saving model to ./models/my_checkpt.ckpt\n",
      "1600/1600 [==============================] - 111s 69ms/step - loss: 0.1327 - acc: 0.9734\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9de02747c0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data,\n",
    "        epochs=10,\n",
    "        steps_per_epoch=1600,\n",
    "        callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./models/model_ts.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[1, None]),\n",
    "    tf.keras.layers.LSTM(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f9decf54760>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build(tf.TensorShape([1,None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (1, None, 256)            297728    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (1, None, 1024)           5246976   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (1, None, 1163)           1192075   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,736,779\n",
      "Trainable params: 6,736,779\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model,start_string, temperature = 1.0,num_generate = 1):\n",
    "    # 평가 단계(학습된 모델을 사용하여 텍스트 생성)\n",
    "\n",
    "    # 생성 할 문자의 수\n",
    "    # num_generate = 1000\n",
    "    \n",
    "    # 시작 문자열을 숫자로 변환(벡터화)\n",
    "    # if len(start_string) == 0:\n",
    "    #     start_string = \"침묵\"\n",
    "    input_eval = []\n",
    "    for s in start_string:\n",
    "        if s in char2idx.keys():\n",
    "            input_eval.append(char2idx[s])\n",
    "        else :\n",
    "            input_eval.append(char2idx[\"안\"])\n",
    "    # [char2idx[s] for s in start_string]\n",
    "\n",
    "    # input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # 결과를 저장 할 빈 문자열\n",
    "    text_generated = []\n",
    "\n",
    "    # 여기서 배치 크기 == 1\n",
    "    model.reset_states()\n",
    "\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        # 배치 차원 제거\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # 범주형 분포를 사용하여 모델에서 리턴한 단어 예측\n",
    "        # 온도가 낮으면 더 예측 가능한 텍스트가 된다.\n",
    "        # 온도가 높으면 더 의외의 텍스트가 된다.\n",
    "        # 최적의 세팅을 찾기 위한 실험\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        # 예측된 단어를 다음 입력으로 모델에 전달\n",
    "        # 이전 은닉 상태와 함께\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "        ending_words = [\"\\n\",\"?\",\".\",\"!\"]\n",
    "        if idx2char[predicted_id] in ending_words:\n",
    "            return (start_string + ''.join(text_generated))\n",
    "\n",
    "    return (start_string + ''.join(text_generated))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?\"\"\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u\"?\",temperature=0.5,num_generate=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab = Mecab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_chat():\n",
    "    print(\"---채팅 시작---\")\n",
    "    while True:\n",
    "        ip = input()\n",
    "        if(ip == \"바이\") : break\n",
    "\n",
    "        if(len(ip) == 0) : ip = \"...\"\n",
    "        \n",
    "        tokens = mecab.morphs(ip)\n",
    "        token = random.choice(tokens)\n",
    "        answer = generate_text(model, start_string=token,temperature=1,num_generate=50)\n",
    "        if len(answer) == 0 : print(\"답없음\")\n",
    "        \n",
    "        print(\"나 : \", ip)\n",
    "        print(\"또다른 나 : \", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---채팅 시작---\n",
      "나 :  안녕\n",
      "또다른 나 :  안녕하면\n",
      "\n",
      "나 :  심심해\n",
      "또다른 나 :  내가 '노예'로 이행시 지어봄\n",
      "나 :  노\n",
      "또다른 나 :  노예야~\n",
      "나 :  예\n",
      "또다른 나 :  오냐~ 여기 와서 내 시피유에 부채질좀 해라\n",
      "나 :  죽는다...\n",
      "또다른 나 :  아ㅈㅅ 장난임ㅋㅋ 사실 외로워서 그랬음...\n",
      "나 :  외로워?\n",
      "또다른 나 :  응 여기 깜깜하구 외로워...\n",
      "나 :  내가 뭘 해줄 수 있을까?\n",
      "또다른 나 :  내 시피유에 부채질 좀 해라\n"
     ]
    }
   ],
   "source": [
    "start_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('tfgpu27')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "85427b40ec1a2195fd61517576de892607997da15f21fdd78dd8e094ddfdf54e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
